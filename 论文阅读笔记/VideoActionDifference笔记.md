<h1>VideoActionDifference笔记</h1>



[原文](https://openreview.net/forum?id=3bcN6xlO6f)



<h2><font color=red>任务定义</font></h2>

两种形式：开放式，封闭式



**封闭式**

给定：描述语句 s

视频对 $(V_A,V_B)$ 

一系列诸如“跳更高”的差异描述集合 ${d_0,d_1,...,d_{k-1}}$

对于一系列状态，模型对k个状态给出一个对应的预测，它是更倾向于视频A还是视频B



*评估方式：*

直接按照是否正确回答来算百分率，乱猜50%，满分100%



**开放式**

和上面的区别在于：而是给定一个整数 $N_{diff}$ ，需要你给出长度不超过 $N_{diff}$  关于差异的陈述D（替代上面给定的差异陈述），然后借此给出预测P



*评估方式*

用大语言模型(LLM)作字符串匹配



<h2><font color=green>数据集</font></h2>



**视频来源**

健身

球运动

潜水

音乐演奏

手术



数据集标注的时候，为了统一，他们是创建了一个体系：

即差异分为几个类别，每次考虑差异的时候从这几个类别上考虑



视频长度几秒到几昏钟不等



<h2><font color=red>方法</font></h2>

**Agent workflow**

![VidDiff(1)](../论文阅读笔记/img/VidDiff(1).png)

*第一步：*在给定动作描述的情况下，生成可能出现的动作候选

也就是说，<font color=red>用LLM将s拆解</font>(差异拆解，而非具体动作拆解)

*第二步：* 还是用LLM，但是是将s一连串动作本身拆解成多个子动作，然后再用CLIP作跨模态检索——根据拆解出来的子动作文本串，CLIP再拿根据差异生成的检索描述文本去检索很匹配的视频片段内容。

得到符合的视频片段内容后

*第三步：*将检索到的视频片段，以及第一步的差异描述，送给VLM来生成答案



```Chinese(dog)
感觉是流程创新，针对差异没有对模型内的创新，怎么说呢
```



这种方式有点多了：

1.找个细分领域，细分问题，没有特别详细数据集（没有新数据集的就被拒了）

2.整数据集

3.使用agent方法的模型，三步走





整体上三步：

**1.打开冰箱**

<font color=blue>拆解</font>

根据问题，将问题给LLM，然后根据具体问题，将它拆解

**2.把大象送进冰箱**

<font color=blue>分派任务</font>

指定各个准备好的模型（根据任务具体情景，准备一系列的模型），让它们完成对应的子问题

**3.关冰箱**

<font color=blue>整合得到答案</font>

生成python代码串联上一步，或者将上面生成的内容送给一个能够整合子任务答案的模型。然后得到答案



<h2><font color=blue>表现</font></h2>



![VidDiff(2)](../论文阅读笔记/img/VidDiff(2).png)

![VidDiff(3)](../论文阅读笔记/img/VidDiff(3).png)



<h3><font color=purple>差异的错误分析</font></h3>

失败的案例都是那种考究细节(fine-grained)的差异

角度，多帧速度分析

（也不奇怪，主要工作和创新其实是在流程：文本下功夫，但是对于视觉内容，就做了一个检索，然后送给现成的VLM，真正视觉上的那种细差异化问题其实是没有去探索的）

![VidDiff(4)](../论文阅读笔记/img/VidDiff(4).png)