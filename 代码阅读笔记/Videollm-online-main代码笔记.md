<font size=8>Videollm-online-mainä»£ç ç¬”è®°</font>



æ¨¡å‹æœ‰ä»€ä¹ˆç»„æˆï¼Ÿ

`LlamaForCausalLM` ä»¥åŠ`LiveMixin` æ„æˆã€‚å‰è€…æ˜¯ä»transformersæ‰çš„ç±»



<font size=5, color=red>**LiveMixin**</font>

è¿™ä¸€å±‚å¹²çš„æ˜¯ä»€ä¹ˆäº‹ï¼Ÿå¯¹LIVEç®—æµ‹è¯„

<img src="../ä»£ç é˜…è¯»ç¬”è®°/img/videollm(1).png" alt="videollm(1)" style="zoom:50%;" />

**stream_evaluate**

é™¤å¼€selfï¼Œæœ‰äº”ä¸ªå½¢å‚ï¼š

`input_ids` å¤šæ¨¡æ€ç¼–ç åçš„tokens

`labels` GT

`frames`

`ignore_token_id` å¤„ç†æ—¶å¿½ç•¥çš„token

`frame_token_interval_threshhold` $\theta$ é«˜äºè¿™ä¸ªé˜€å€¼æ—¶EOSæ‰ç®—



*ç¬¬ä¸€æ­¥*

```
turn_stops = ((input_id == self.config.eos_token_id).nonzero() + 1)[:,0].tolist()
turn_starts = [0] + turn_stops[:-1]
num_turns = len(turn_starts)
```

å¤šè½®å¯¹è¯ï¼Œè·å–æ¯ä¸€è½®çš„å¼€å§‹ä¸ç»“æŸçš„ä½ç½®

è·å–EOSçš„ä½ç½®å¹¶+1ï¼Œæˆä¸ºç»“æŸçš„ä½ç½®

æ¯ä¸€è½®ç»“æŸçš„ä½ç½®ä¹Ÿæ˜¯ä¸‹ä¸€è½®å¼€å§‹çš„ä½ç½®ï¼Œæ‰€ä»¥è¦åŠ ä¸Š0è¿™ä¸ªæ²¡æœ‰åŒ…æ‹¬è¿›å»çš„ä½ç½®ä»¥åŠæœ€åä¸€è½®çš„ç»“å°¾



*ç¬¬äºŒæ­¥*

```
outputs = self.forward(input_ids=input_ids, frames=frames, return_dict=True, use_cache=True)
logit, past_key_values = outputs.logits[0], outputs.past_key_values
```

å‘å‰ä¼ æ’­ï¼Œä»tokensè¿‡LIVE frameworkç„¶åå¾—åˆ°å®ƒçš„logins(æœªè¿›è¡Œå½’ä¸€åŒ–å¾—åˆ°çš„è¾“å‡º)ä»¥åŠkv



*ç¬¬ä¸‰æ­¥*

æœ‰å››ä¸ªæŒ‡æ ‡metricsï¼š

```
lm_ppls, frame_diffs, fluencies, lm_correctness = [], [], [], []
```

*LM-PPL* è¶Šä½è¶Šå¥½ï¼ˆè¯­è¨€å›°æƒ‘åº¦ï¼‰

*TimeDiff* äº‹ä»¶å¯¹é½èƒ½åŠ›

åäºŒè€…å«ä¹‰å¦‚å…¶å



```
 for r, (turn_start, turn_stop) in enumerate(zip(turn_starts, turn_stops)):
            ## 3.1. we only have two losses: stream loss on frame tokens, and lm loss. prepare corresponding mask according two losses
            turn_label = label[turn_start:turn_stop]
            turn_learn_mask = turn_label != ignore_token_id
            if not turn_learn_mask.any():
                continue
            turn_logit = logit[turn_start:turn_stop]
            turn_input_id = input_id[turn_start:turn_stop]
            turn_v_mask = turn_input_id == v_placeholder_id
            turn_num_frames = turn_v_mask.sum() // frame_num_tokens
            turn_stream_mask = turn_v_mask & turn_learn_mask
            turn_lm_mask = turn_learn_mask & ~turn_stream_mask
```

å¯¹æ¯ä¸€è½®å¤„ç†ï¼š

å‰ç¼€åŠ turn éƒ½æ˜¯æŒ‡è¿™ä¸€è½®æ¬¡çš„å†…å®¹

`turn_learn_mask` é™¤å¼€éœ€è¦å¿½ç•¥tokenåçš„ï¼Œå¾…å¤„ç†çš„labelèŒƒå›´

`turn_v_mask` å–å‡ºè¾“å…¥çš„æ‰€æœ‰tokenä¸­æ˜¯è§†è§‰å ä½ç¬¦çš„token

`turn_num_frames` è¯¥è½®æ¬¡æ‰€æœ‰è§†è§‰å ä½ç¬¦tokençš„æ€»å’Œ / æ¯ä¸€å¸§çš„tokenæ•°ç›®

`turn_stream_mask` åœ¨å¾…å¤„ç†çš„èŒƒå›´å†…çš„ï¼Œå±äºè§†è§‰tokençš„å†…å®¹ï¼ˆæ˜¯å–äº¤é›†ï¼‰

`turn_lm_mask` åœ¨å¾…å¤„ç†èŒƒå›´å†…çš„ï¼Œä½†æ˜¯ä¸å±äºè§†è§‰tokençš„éƒ½å±äºstandard languagemodeling (LM) losså¯¹åº”çš„maskä½ç½®



```
 ## 3.2 ppl, offline metric
            if turn_lm_mask.any():
                turn_lm_masked_logit, turn_lm_masked_label = turn_logit[turn_lm_mask], turn_label[turn_lm_mask]
                lm_ppl = torch.nn.functional.cross_entropy(turn_lm_masked_logit, turn_lm_masked_label).exp()
                lm_ppls.append(lm_ppl)
                turn_lm_masked_wrong_mask = turn_lm_masked_logit.argmax(dim=-1) != turn_lm_masked_label
                if turn_lm_masked_wrong_mask.any():
                    num_lm_correct_tokens = turn_lm_masked_wrong_mask.nonzero()[0,0]
                else:
                    num_lm_correct_tokens = (~turn_lm_masked_wrong_mask).sum()
                lm_correctness.append(num_lm_correct_tokens / turn_lm_masked_label.numel())
```

ç®—è¯­è¨€å›°æƒ‘åº¦å’Œæ­£ç¡®ç‡

`num_lm_correct_tokens` æ˜¯å–æ¯ä¸ªbatchç¬¬ä¸€ä¸ªé¢„æµ‹é”™è¯¯çš„ä¸‹æ ‡ï¼ˆç”±äºä¸‹æ ‡ä»0å¼€å§‹ï¼‰ï¼Œæ‰€ä»¥å…¶å®ä¹Ÿæ˜¯è¿ç»­é¢„æµ‹æ­£ç¡®çš„tokenæ•°ç›®

ï¼ˆéš¾é“ä¸èƒ½æˆ‘è¿ç»­é¢„æµ‹é”™è¯¯åï¼Œç”±é¢„æµ‹æ­£ç¡®äº†ä¸ç®—å—ï¼Ÿæ¯”å¦‚ï¼šGTï¼šå¼ ä¸‰æ¥åˆ°å­¦æ ¡å®éªŒå®¤ä¸Šæœºï¼Œé¢„æµ‹ï¼šå¼ ä¸‰æ¥åˆ°ç½‘å§å–Šç½‘ç®¡ä¸Šæœºï¼Œé‚£ä¹ˆåªç®—å‰4ä¸ªé¢„æµ‹æ­£ç¡®ï¼Œæœ€åä¸¤ä¸ªä¸ç®—ï¼‰



```
 ## 3.3. frame_diff (will be casted to time_diff in compute_metrics)
            if turn_stream_mask.any():
                ## 3.3.1: reply before (at) turn_num_frames
                turn_score = turn_logit.softmax(dim=-1)
                turn_stream_masked_score = turn_score[turn_stream_mask]
                if frame_token_interval_threshold > 0:
                    lower_threshold_mask = turn_stream_masked_score[:, frame_token_interval_id] < frame_token_interval_threshold
                    turn_stream_masked_score[lower_threshold_mask] = 0
                turn_stream_masked_pred_mask = turn_stream_masked_score.argmax(dim=-1) != frame_token_interval_id
                if turn_stream_masked_pred_mask.any():
                    frame_diff = turn_stream_mask.sum() - turn_stream_masked_pred_mask.nonzero()[0,0] - 1
                else:
                ......
                ......
```



### **`frame_diff` çš„å«ä¹‰**

- **å®šä¹‰**ï¼šæ¨¡å‹é¢„æµ‹å“åº”çš„å¸§ä½ç½®ä¸ç†æƒ³æ—¶é—´ç‚¹ä¹‹é—´çš„å·®è·ï¼ˆä»¥å¸§æ•°ä¸ºå•ä½ï¼‰ã€‚ä¹Ÿå°±æ˜¯å¯¹äºå›¾ä¸­é»„è‰²çš„å¸§å®ƒæ˜¯å¦è¢«æ”¾å€’äº†æ­£ç¡®ä½ç½®ï¼Œå¦‚æœå›¾ä¸­çš„AIï¼ˆtraining targeté‚£è¡Œ)è¢«æ”¾å€’äº†å‰é¢ï¼ˆå·¦è¾¹ğŸ‘ˆï¼‰ï¼Œé‚£ä¹ˆå°±æ˜¯æå‰å“åº”ï¼Œ`frame_diff` å¤§äº0ï¼Œå¦‚æœå¯¹é½äº†ï¼ˆä¸‹å›¾ä¸­çš„ä½ç½®ï¼‰ï¼Œé‚£å°±æ˜¯0ï¼Œå¦‚æœæ”¾åˆ°åé¢ï¼ˆå³è¾¹ğŸ‘‰ï¼‰ï¼Œè·¨è½®æ¬¡å“åº”ï¼Œé‚£å°±æ˜¯å»¶è¿Ÿå“åº” `frame_diff` å°äº0

- **Anyway,summay:**

  **`frame_diff = 0`**ï¼šæ¨¡å‹åœ¨ç†æƒ³æ—¶é—´ç‚¹å“åº”ï¼ˆå®Œç¾å¯¹é½)

  **`frame_diff > 0`**ï¼šæ¨¡å‹æå‰å“åº”ï¼ˆå¯èƒ½æ¼æ‰åç»­äº‹ä»¶ï¼‰

  **`frame_diff < 0`**ï¼šæ¨¡å‹å»¶è¿Ÿå“åº”ï¼ˆå¯èƒ½é”™è¿‡å½“å‰äº‹ä»¶ï¼‰

- æœ€åå¯¹`frame_diff`å–ç»å¯¹å€¼ï¼Œè®­ç»ƒç›®æ ‡å½“ç„¶å°±æ˜¯è®©`frame_diff` è¶Šå°è¶Šå¥½

**æ³¨é‡Šä¸­è¡¥å……äº†ï¼šå…¶å®è¿™ä¸ªframe_diffå°±æ˜¯paperé‡Œé¢è®¡ç®—çš„Time Difference (TimeDiff) metric**



<font color=blue>å…ˆçœ‹ `frame_diff>=0` çš„æƒ…å†µï¼š</font>

`turn_stream_mask` æ˜¯æ¯ä¸€è½®è¾“å…¥çš„è§†é¢‘å¸§çš„æ©ç 

å†æ¥çœ‹ `turn_stream_masked_pred_mask` 

```
turn_score = turn_logit.softmax(dim=-1)
turn_stream_masked_score = turn_score[turn_stream_mask]
lower_threshold_mask = turn_stream_masked_score[:, frame_token_interval_id] < frame_token_interval_threshold
turn_stream_masked_score[lower_threshold_mask] = 0
turn_stream_masked_pred_mask = turn_stream_masked_score.argmax(dim=-1) != frame_token_interval_id
```

å®ƒæ˜¯å¯¹ç»è¿‡LIVE frameworkåå¾—åˆ°çš„è¾“å‡ºlogitsï¼Œsoftmaxï¼Œè®¡ç®—å‡ºæ¦‚ç‡åˆ†å¸ƒï¼Œç„¶åé€‰å–åº”è¯¥é¢„æµ‹EOSçš„ä½ç½®å¹¶æ»¤å»ä½äºé˜€å€¼ $P^{[EOS]}<\theta$ 

çš„éƒ¨åˆ†ï¼ˆç½®0ï¼‰ï¼Œç„¶åå†å¾—åˆ°åº”è¯¥å“åº”çš„æ©ç çš„ä½ç½®(ä¸æ˜¯EOSçš„ï¼Œä¸”åœ¨è§†è§‰ä¸Šçš„å³æ˜¯æ¨¡å‹åº”è¯¥ç”Ÿæˆç›¸åº”çš„ä½ç½®)ï¼š

`turn_stream_masked_pred_mask = turn_stream_masked_score.argmax(dim=-1) != frame_token_interval_id`

ä¹Ÿå°±æ˜¯å›¾ä¸­çš„é»„è‰²çš„éƒ¨åˆ†ï¼š

<img src="../ä»£ç é˜…è¯»ç¬”è®°/img/videollm(2).png" alt="videollm(2)" style="zoom:50%;" />



è‹¥æ¨¡å‹åœ¨æŸä¸ªå¸§çš„æœ€åä¸€ä¸ª token å¤„é¢„æµ‹äº† **é EOS æ ‡è®°**ï¼ˆå³å†³å®šå“åº”ï¼‰ï¼š`frame_diff` = å½“å‰è½®æ¬¡çš„æ€»å¸§æ•° - é¢„æµ‹å“åº”çš„å¸§ä½ç½® - 1
ï¼ˆä¾‹å¦‚ï¼Œæ€»å¸§æ•°ä¸º 5ï¼Œåœ¨ç¬¬ 2 å¸§å“åº”ï¼Œåˆ™ `frame_diff = 5 - 2 - 1 = 2`ï¼Œè¡¨ç¤ºå»¶è¿Ÿäº† 2 å¸§ï¼‰ã€‚



è‹¥æ¨¡å‹æœªåœ¨å½“å‰è½®æ¬¡å“åº”ï¼š

æ¨¡æ‹Ÿæ·»åŠ åç»­å¸§çš„è¾“å…¥ï¼Œæ£€æŸ¥æ¨¡å‹æ˜¯å¦åœ¨åç»­å¸§ä¸­å“åº”ã€‚

è‹¥åœ¨åç»­ç¬¬ `k` å¸§å“åº”ï¼Œåˆ™ `frame_diff = -k`ï¼ˆè´Ÿå€¼è¡¨ç¤ºå»¶è¿Ÿåˆ°åç»­å¸§ï¼‰ã€‚





<font color=red>å†çœ‹`frame_diff<0`çš„æƒ…å†µ</font>

ä½œè€…é’¦å®šçš„æœ€å¤æ‚çš„éƒ¨åˆ†ï¼šï¼ˆ115-149ï¼‰

å³æ˜¯æ²¡æœ‰åœ¨å½“å‰è½®æ¬¡å“åº”ï¼Œè€Œæ˜¯åé¢çš„è½®æ¬¡å“åº”ï¼Œå»¶è¿Ÿå“åº”

```
                else:
                    ## 3.3.2: the most complex part,reply after turn_num_frames. we assume the 'assistant: ...' not exists
                    turn_last_stream_idx = turn_stream_mask.nonzero()[-1,0]
                    past_key_values_before_assistant = self.trim_past_key_values(past_key_values, 0, turn_start + turn_last_stream_idx + 1)
                    if r == num_turns - 1: # no future frame. we assume the model should receive a signal when streaming ends (e.g. close button).
                        frame_diff = zero
                    else:
                        next_turn_num_frames = (input_id[turn_starts[r+1]:turn_stops[r+1]] == v_placeholder_id).sum() // frame_num_tokens
                        to_append_num_frames = min(next_turn_num_frames, turn_num_frames - 1) # avoid bias. current as center, two equal left/right side
                        if to_append_num_frames == 0:
                            frame_diff = zero
                        else:
                            to_append_frames = frames[past_num_frames+turn_num_frames:past_num_frames+turn_num_frames+to_append_num_frames]
                            frame_placeholder = [v_placeholder_id] * frame_num_tokens
                            if use_interval:
                                frame_placeholder = [frame_token_interval_id] + frame_placeholder
                            to_append_input_id = torch.tensor(frame_placeholder * to_append_num_frames, dtype=torch.long, device=device)
                            to_append_logit = self.forward(
                                input_ids=to_append_input_id[None],
                                past_key_values=past_key_values_before_assistant,
                                frames=to_append_frames,
                                return_dict=True, use_cache=True
                            ).logits[0]
                            # we only use the last idx of each frame
                            idxs = torch.arange(len(frame_placeholder)-1, len(to_append_input_id), len(frame_placeholder), device=device)
                            to_append_score = to_append_logit[idxs].softmax(dim=-1)
                            if frame_token_interval_threshold > 0:
                                lower_threshold_mask = to_append_score[:, frame_token_interval_id] < frame_token_interval_threshold
                                to_append_score[lower_threshold_mask] = 0
                            to_append_score_pred_mask = to_append_score.argmax(dim=-1) != frame_token_interval_id
                            if to_append_score_pred_mask.any():
                                frame_diff = -(to_append_score_pred_mask.nonzero()[0,0] + 1)
                            else:
                                frame_diff = -to_append_num_frames
frame_diffs.append(frame_diff.abs())
```

ä»ä¸æ˜¯æœ€åä¸€è½®çš„elseçš„ä»£ç å¼€å§‹åˆ†æï¼š

`next_turn_num_frames = (input_id[turn_starts[r+1]:turn_stops[r+1]] == v_placeholder_id).sum() // frame_num_tokens`

`input_id` æ˜¯åŒ…å«äº†æ–‡æœ¬å’Œè§†è§‰çš„tokensï¼Œç„¶åå–å‡ºä¸‹ä¸€è½®å¼€å§‹ä¸ç»“æŸçš„ä¸­æ˜¯è§†è§‰çš„æ‰€æœ‰tokenæ€»å’Œï¼Œç„¶åé™¤ä»¥æ¯ä¸€å¸§å¯¹åº”çš„tokenæ•°ç›®ï¼Œå¾—åˆ° `next_turn_num_frames` ä¸‹ä¸€è½®å¯¹è¯çš„å¸§æ•°





ä¸‹ä¸€æ­¥å–minå¾—åˆ°ä¸‹ä¸€è½®æ·»åŠ çš„å¸§æ•°

ä¸ºäº†ä¿è¯å½“å‰çš„å¸§ä¸ºä¸­å¿ƒï¼Œå®ƒçš„å·¦ä¸å³å°½å¯èƒ½å¹³è¡¡ï¼Œä¹Ÿå°±æ˜¯è¯´å·¦è¾¹çš„å½“å‰è½®ï¼ˆå»é™¤å½“å‰çš„å¸§ï¼Œ-1ï¼‰çš„æ•°ç›®è¦å’Œå³è¾¹ï¼Œè¦æœªæ¥æ·»åŠ çš„æ•°ç›®ç›¸æ¥è¿‘ï¼ˆä½†æ˜¯ä¸ºä»€ä¹ˆè¦ä¿æŒå¹³è¡¡ï¼Ÿå±•ç¤ºè¿˜æ²¡æ˜ç™½ï¼‰

é¦–å…ˆ `turn_num_frames` æ€ä¹ˆå¾—æ¥çš„ï¼š

```
turn_input_id = input_id[turn_start:turn_stop]
turn_v_mask = turn_input_id == v_placeholder_id
turn_num_frames = turn_v_mask.sum() // frame_num_tokens
```

åˆå¹¶æˆï¼š

`turn_num_frames = (input_id[turn_start:turn_stop]==v_placeholder_id).sum()//frame_num_tokens`

å†çœ‹ `next_turn_num_frames`

`next_turn_num_frames = (input_id[turn_starts[r+1]:turn_stops[r+1]] == v_placeholder_id).sum() // frame_num_tokens`

äºŒè€…æ¯”å¤§å°ï¼Œå–å…¶minï¼Œå°±æ˜¯çœ‹ç´¢å¼•æ‰€æ¶µç›–çš„å¤§å°ï¼Œ`turn_num_frames` çœ‹çš„æ˜¯å½“å‰è½®æ¬¡çš„å¤§å°ï¼Œ`next_turn_num_frames` çœ‹çš„æ˜¯ä¸‹ä¸€æ¬¡è½®æ¬¡çš„å¤§å°



ç„¶åå°±æ˜¯æ·»åŠ ï¼š

```
                 if to_append_num_frames == 0:
                            frame_diff = zero
                        else:
                            to_append_frames = frames[past_num_frames+turn_num_frames:past_num_frames+turn_num_frames+to_append_num_frames]
                            frame_placeholder = [v_placeholder_id] * frame_num_tokens
                            if use_interval:
                                frame_placeholder = [frame_token_interval_id] + frame_placeholder
                            to_append_input_id = torch.tensor(frame_placeholder * to_append_num_frames, dtype=torch.long, device=device)
                            to_append_logit = self.forward(
                                input_ids=to_append_input_id[None],
                                past_key_values=past_key_values_before_assistant,
                                frames=to_append_frames,
                                return_dict=True, use_cache=True
                            ).logits[0]
```



ç„¶åä½œè€…è¿™é‡Œé»˜è®¤ï¼Œæ¯ä¸€å¸§ä»¥æ‰€æœ‰å¸§æœ€åä¸€å¸§ä¸ºç´¢å¼•

è¿™ä¸€é€šæ“ä½œå°±æ˜¯æ‰©å±•å¸§ï¼Œç„¶åå†å‘å‰ä¼ æ’­ä¸€æ¬¡ï¼Œç„¶åå†åƒä¸Šé¢ (`frame_diff>=0` çš„æƒ…å†µä¸€æ ·)ä¾è‘«èŠ¦ç”»ç“¢

```
                            idxs = torch.arange(len(frame_placeholder)-1, len(to_append_input_id), len(frame_placeholder), device=device)
                            to_append_score = to_append_logit[idxs].softmax(dim=-1)
                            if frame_token_interval_threshold > 0:
                                lower_threshold_mask = to_append_score[:, frame_token_interval_id] < frame_token_interval_threshold
                                to_append_score[lower_threshold_mask] = 0
                            to_append_score_pred_mask = to_append_score.argmax(dim=-1) != frame_token_interval_id
                            if to_append_score_pred_mask.any():
                                frame_diff = -(to_append_score_pred_mask.nonzero()[0,0] + 1)
                            else:
                                frame_diff = -to_append_num_frames
```

æœ€åå–ç»å¯¹å€¼ï¼Œç®—å‡º`frame_diff`



<font color=blue>*fast_greedy_generate*</font>

```
def fast_greedy_generate(*, model: LiveMixin, inputs_embeds: torch.Tensor, past_key_values: Cache, eos_token_id: int, inplace_output_ids: torch.Tensor):
    for i in range(inplace_output_ids.size(1)):
        outputs = model(inputs_embeds=inputs_embeds, past_key_values=past_key_values, use_cache=True)
        past_key_values = outputs.past_key_values
        new_token_id = outputs.logits[:, -1:].argmax(dim=-1)
        inplace_output_ids[:, i] = new_token_id
        if new_token_id == eos_token_id:
            break
        inputs_embeds = model.get_input_embeddings()(new_token_id)
    return inplace_output_ids[:, :i+1], past_key_values
```

`inplace_output_ids` æ”¶é›†ç”Ÿæˆçš„tokenï¼ˆLIVE frameworkçš„ç”Ÿæˆï¼‰





<font size=5, color=blue>**LiveInfer**</font>



<font color=red>*_call_for_response* å‡½æ•°ï¼š</font>

```
def _call_for_response(self, video_time, query):
        if query is not None:
            self.last_ids = self.tokenizer.apply_chat_template([{'role': 'user', 'content': query}], add_stream_query_prompt=True, add_generation_prompt=True, return_tensors='pt').to('cuda')
        else:
            assert self.last_ids == 933, f'{self.last_ids} != 933' # HACK, 933 = ]\n
            self.last_ids = self._added_stream_generation_ids
        inputs_embeds = self.model.get_input_embeddings()(self.last_ids)
        output_ids, self.past_key_values = fast_greedy_generate(model=self.model, inputs_embeds=inputs_embeds, past_key_values=self.past_key_values, eos_token_id=self.eos_token_id, inplace_output_ids=self.inplace_output_ids)
        self.last_ids = output_ids[:, -1:]
        if query:
            query = f'(Video Time = {video_time}s) User: {query}'
        response = f'(Video Time = {video_time}s) Assistant:{self.tokenizer.decode(output_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)}'
        return query, response
```

å…ˆçœ‹æŸ¥è¯¢æ˜¯å¦æœ‰ï¼Œæœ‰çš„è¯å°†`last_ids`ä½¿ç”¨å¯¹è¯æ¨¡ç‰ˆï¼Œå½¢æˆå³å°†æŸ¥è¯¢çš„ä¸Šä¸‹é—®ï¼Œ æ²¡æœ‰çš„è¯çœ‹æœ€è¿‘çš„tokenæ˜¯å¦ä¸ºç‰¹æ®Šæ ‡è®°çš„tokenï¼Œ933 æ˜¯ ]\n çš„æ ‡è®°IDï¼Œç”¨äºç¡®ä¿ä¸Šä¸€ä¸ªå¯¹è¯å·²æ­£ç¡®ç»“æŸï¼Œå¦‚æœæ£€æŸ¥é€šè¿‡ï¼Œåˆ™å°† last_ids é‡ç½®ä¸ºæ–°çš„ç”Ÿæˆæç¤ºæ ‡è®°ï¼Œä¸ºä¸‹ä¸€è½®å¯¹è¯åšå‡†å¤‡,ç”¨äºç¡®ä¿å¯¹è¯æµçš„æ­£ç¡®è½¬æ¢ã€‚ï¼ˆä»£ç ä¸­çš„ `last_ids` æ˜¯å­˜å‚¨æœ€è¿‘ç”Ÿæˆçš„ä¸€æ¬¡tokens)





```
inputs_embeds = self.model.get_input_embeddings()(self.last_ids)
output_ids, self.past_key_values = fast_greedy_generate(model=self.model, inputs_embeds=inputs_embeds, past_key_values=self.past_key_values, eos_token_id=self.eos_token_id,inplace_output_ids=self.inplace_output_ids)
self.last_ids = output_ids[:, -1:]
```

ç”Ÿæˆæ–°ä¸€è½®å‡†å¤‡å›ç­”çš„tokenåï¼Œå°†æœ€åä¸€ä¸ªtokenå…¨å­˜åˆ° `last_ids` ä¸­

```
if query:
         query = f'(Video Time = {video_time}s) User: {query}'
         response = f'(Video Time = {video_time}s) Assistant {self.tokenizer.decode(output_ids[0],skip_special_tokens=True, clean_up_tokenization_spaces=True)}'
        return query, response
```

è§£ç è¾“å‡ºç›¸å…³å†…å®¹



<font color=red>*_call_for_streamingå‡½æ•°*</font>

å¤„ç†frame_embeddingçš„é˜Ÿåˆ—ä»¥åŠqueryé˜Ÿåˆ—äºŒè€…çš„é˜Ÿå¤´ï¼ŒæŸ¥è¯¢çš„é˜Ÿå¤´åœ¨è§†é¢‘å¸§embeddingä¹‹å‰ï¼Œç›´æ¥å…ˆè¿”å›queryå¯¹åº”çš„æ—¶é—´æˆ³åŠå…¶å†…å®¹ï¼š

```
 if self.query_queue and self.frame_embeds_queue[0][0] > self.query_queue[0][0]:
                video_time, query = self.query_queue.popleft()
                return video_time, query
```



åä¹‹åˆ™éœ€è¦å…ˆpopè§†é¢‘å¸§embeddingï¼Œç„¶åç¼–ç å‘å‰ä¼ æ’­frame_embeddingï¼š

```
video_time, frame_embeds = self.frame_embeds_queue.popleft()
            if not self.past_key_values:
                self.last_ids = self._start_ids
            elif self.last_ids == self.eos_token_id:
                self.last_ids = torch.cat([self.last_ids, self._added_stream_prompt_ids], dim=1)
            inputs_embeds = torch.cat([
                self.model.get_input_embeddings()(self.last_ids).view(1, -1, self.hidden_size),
                frame_embeds.view(1, -1, self.hidden_size),
            ], dim=1)
            outpus = self.model(inputs_embeds=inputs_embeds, use_cache=True, past_key_values=self.past_key_values)
            self.past_key_values = outputs.past_key_values
```

å…¶ä¸­ï¼šå¦‚æœæ²¡æœ‰past_kvæ²¡æœ‰ï¼Œå³æ²¡æœ‰å†å²ï¼Œå°±æ˜¯å¼€å¤´ï¼Œå³start_ids



ç„¶åå†æŸ¥çœ‹åˆšåˆšå‡ºé˜Ÿ(frame_embeds)çš„æ—¶é—´æˆ³ï¼Œç„¶åå‡ºé˜Ÿçš„æ—¶é—´æˆ³å¦‚æœæ—©äºæŸ¥è¯¢é˜Ÿåˆ—çš„é˜Ÿå°¾ï¼Œé‚£ä¹ˆè¿”å›è¿™ä¸ªæŸ¥è¯¢æ—¶é—´æˆ³å’Œé—®é¢˜ï¼ˆå³å…ˆå¤„ç†æ—¶é—´æˆ³ï¼Œå†å›ç­”ï¼‰



```
 next_score = outputs.logits[:,-1:].softmax(dim=-1)
            if next_score[:,:,self.frame_token_interval_id] < self.frame_token_interval_threshold:
                next_score[:,:,self.frame_token_interval_id].zero_()
            self.last_ids = next_score.argmax(dim=-1)
            if self.last_ids != self.frame_token_interval_id: 
                return video_time, None
        return None, None
```

å¦‚æœæœ€è¿‘çš„tokenéœ€è¦å“åº”ï¼Œé‚£ä¹ˆä»€ä¹ˆéƒ½ä¸è¿”å›ï¼Œå¦åˆ™åªè¿”å›è§†é¢‘çš„æ—¶é—´æˆ³

å¦‚æœé˜Ÿå°¾å¤šä¸ªæŸ¥è¯¢æ—¶é—´æˆ³éƒ½æ—©äºè§†é¢‘æµé˜Ÿå¤´çš„æ—¶é—´æˆ³ï¼Œä»ç„¶è¿”å›



**æ€»ç»“**ï¼šåªæœ‰è§†é¢‘æµæ—¶é—´æˆ³æ™šäºæŸ¥è¯¢æ—¶é—´æˆ³(é˜Ÿå¤´)ï¼Œé‚£ä¹ˆè¿”å›é—®é¢˜ä»¥åŠå¯¹åº”æ—¶é—´æˆ³ä»¥æ­¤å“åº”ã€‚å¦åˆ™ï¼Œå…ˆå‘å‰ä¼ æ’­frame_embedding

å¦‚æœè§†é¢‘æµæ—¶é—´æˆ³åˆšå¥½ç­‰äºæŸ¥è¯¢æ—¶é—´æˆ³ï¼Œé‚£ä¹ˆä»ç„¶è¿”å›é—®é¢˜ä»¥åŠå¯¹åº”æ—¶é—´æˆ³ï¼ˆå’Œå‰è€…çš„åŒºåˆ«å°±æ˜¯å¤šä¸€ä¸ªå¯¹è§†é¢‘ç¼–ç ï¼Œæ›´æ–°kvçš„è¿‡ç¨‹ï¼‰

å‰©ä¸‹çš„æƒ…å†µå°±æ˜¯é˜Ÿå°¾ä¸­æœ€æ—©çš„é—®é¢˜æ˜¯æ™šäºæœ€æ—©çš„frame_embeddingçš„æ—¶é—´ï¼Œé‚£ä¹ˆé™¤å¼€è¿™ä¸ªæ—¶é—´æˆ³æ˜¯éœ€è¦å“åº”çš„æ—¶é—´èŠ‚ç‚¹ï¼ˆè¿™ä¸ªæ—¶é—´èŠ‚ç‚¹ä¸æ˜¯æŸ¥è¯¢é—®é¢˜é˜Ÿåˆ—ä¸­å·²æœ‰çš„ï¼Œè€Œæ˜¯å…ˆå‰æŸä¸ªé—®é¢˜LIVE frameworké¢„æµ‹çš„ï¼‰éœ€è¦æ²‰é»˜ï¼Œå•¥éƒ½ä¸è¿”å›ï¼Œå¦åˆ™å°±è¿”å›è¯¥frame_embeddingå¯¹åº”çš„æ—¶é—´æˆ³



<font color=red>*input_video_stream*</font>

` frame_idx = int(video_time * self.frame_fps)`è®¡ç®—å½“å‰å¸§çš„ç´¢å¼•ï¼ˆæ—¶é—´æˆ³ä¹˜ä¸Šå¸§ç‡ï¼‰



```
ranger = range(self.last_frame_idx + 1, frame_idx + 1)
frames_embeds = self.model.visual_embed(self.video_tensor[ranger]).split(self.frame_num_tokens)
```

è®¡ç®—ç´¢å¼•èŒƒå›´ï¼Œsplitæ˜¯å¯¹tokenåˆ†å‰²

` self.frame_embeds_queue.extend([(r / self.frame_fps, frame_embeds) for r, frame_embeds in zip(ranger, frames_embeds)])`

ç¬¬ä¸€é¡¹æ˜¯æ—¶é—´æˆ³,å°†äºŒå…ƒç»„ï¼šæ—¶é—´æˆ³ï¼Œå¸§embeddingåŠ å…¥é˜Ÿåˆ—

